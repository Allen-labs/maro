{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "This notebook demonstrates how to use MARO's reinforcement learning (RL) toolkit to solve the container inventory management ([CIM](https://maro.readthedocs.io/en/latest/scenarios/container_inventory_management.html)) problem. It is formalized as a multi-agent reinforcement learning problem, where each port acts as a decision agent. The agents take actions independently, e.g., loading containers to vessels or discharging containers from vessels.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [State Shaper](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#shapers)\n",
    "\n",
    "State shaper converts the environment observation to the model input state which includes temporal and spatial information. For this scenario, the model input state includes: \n",
    "\n",
    "- Temporal information, it includes the past week's information of ports and vessels, such as shortage on port and remaining space on vessel. \n",
    "\n",
    "- Spatial information, it includes the related downstream port features.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maro.rl import StateShaper\n",
    "\n",
    "\n",
    "class CIMStateShaper(StateShaper):\n",
    "    def __init__(self, *, look_back, max_ports_downstream, port_attributes, vessel_attributes):\n",
    "        super().__init__()\n",
    "        self._look_back = look_back\n",
    "        self._max_ports_downstream = max_ports_downstream\n",
    "        self._port_attributes = port_attributes\n",
    "        self._vessel_attributes = vessel_attributes\n",
    "        self._dim = (look_back + 1) * (max_ports_downstream + 1) * len(port_attributes) + len(vessel_attributes)\n",
    "\n",
    "    def __call__(self, decision_event, snapshot_list):\n",
    "        tick, port_idx, vessel_idx = decision_event.tick, decision_event.port_idx, decision_event.vessel_idx\n",
    "        ticks = [tick - rt for rt in range(self._look_back-1)]\n",
    "        future_port_idx_list = snapshot_list[\"vessels\"][tick: vessel_idx: 'future_stop_list'].astype('int')\n",
    "        port_features = snapshot_list[\"ports\"][ticks: [port_idx] + list(future_port_idx_list): self._port_attributes]\n",
    "        vessel_features = snapshot_list[\"vessels\"][tick: vessel_idx: self._vessel_attributes]\n",
    "        state = np.concatenate((port_features, vessel_features))\n",
    "        return str(port_idx), state\n",
    "    \n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Action Shaper](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#shapers)\n",
    "\n",
    "Action shaper is used to convert an agent's model output to an environment executable action. For this specific scenario, the output is a discrete index that corresponds to a percentage indicating the fraction of containers to be loaded to or discharged from the arriving vessel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.rl import ActionShaper\n",
    "from maro.simulator.scenarios.cim.common import Action\n",
    "\n",
    "\n",
    "class CIMActionShaper(ActionShaper):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__()\n",
    "        self._action_space = action_space\n",
    "        self._zero_action_index = action_space.index(0)\n",
    "\n",
    "    def __call__(self, model_action, decision_event, snapshot_list):\n",
    "        assert 0 <= model_action < len(self._action_space)\n",
    "        \n",
    "        scope = decision_event.action_scope\n",
    "        tick = decision_event.tick\n",
    "        port_idx = decision_event.port_idx\n",
    "        vessel_idx = decision_event.vessel_idx\n",
    "        port_empty = snapshot_list[\"ports\"][tick: port_idx: [\"empty\", \"full\", \"on_shipper\", \"on_consignee\"]][0]\n",
    "        vessel_remaining_space = snapshot_list[\"vessels\"][tick: vessel_idx: [\"empty\", \"full\", \"remaining_space\"]][2]\n",
    "        early_discharge = snapshot_list[\"vessels\"][tick:vessel_idx: \"early_discharge\"][0]\n",
    "     \n",
    "        if model_action < self._zero_action_index:\n",
    "            # The number of loaded containers must be less thean the vessel's remaining space.\n",
    "            actual_action = max(round(self._action_space[model_action] * port_empty), -vessel_remaining_space)\n",
    "        elif model_action > self._zero_action_index:\n",
    "            # In the case of an early discharge event, we need to subtract the early discharge amount from the expected \n",
    "            # discharge quote.   \n",
    "            plan_action = self._action_space[model_action] * (scope.discharge + early_discharge) - early_discharge\n",
    "            actual_action = round(plan_action) if plan_action > 0 else round(self._action_space[model_action] * scope.discharge)\n",
    "        else:\n",
    "            actual_action = 0\n",
    "\n",
    "        return Action(vessel_idx, port_idx, actual_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Experience Shaper](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#shapers)\n",
    "\n",
    "Experience shaper is used to convert an episode trajectory to trainable experiences for RL agents. For this specific scenario, the reward is a linear combination of fulfillment and shortage in a limited time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from maro.rl import ExperienceShaper\n",
    "\n",
    "\n",
    "class TruncatedExperienceShaper(ExperienceShaper):\n",
    "    def __init__(self, *, time_window: int, time_decay_factor: float, fulfillment_factor: float,\n",
    "                 shortage_factor: float):\n",
    "        super().__init__(reward_func=None)\n",
    "        self._time_window = time_window\n",
    "        self._time_decay_factor = time_decay_factor\n",
    "        self._fulfillment_factor = fulfillment_factor\n",
    "        self._shortage_factor = shortage_factor\n",
    "\n",
    "    def __call__(self, trajectory, snapshot_list):\n",
    "        experiences_by_agent = {}\n",
    "        for i in range(len(trajectory) - 1):\n",
    "            transition = trajectory[i]\n",
    "            agent_id = transition[\"agent_id\"]\n",
    "            if agent_id not in experiences_by_agent:\n",
    "                experiences_by_agent[agent_id] = defaultdict(list)\n",
    "            \n",
    "            experiences = experiences_by_agent[agent_id]\n",
    "            experiences[\"state\"].append(transition[\"state\"])\n",
    "            experiences[\"action\"].append(transition[\"action\"])\n",
    "            experiences[\"reward\"].append(self._compute_reward(transition[\"event\"], snapshot_list))\n",
    "            experiences[\"next_state\"].append(trajectory[i+1][\"state\"])\n",
    "\n",
    "        return experiences_by_agent\n",
    "\n",
    "    def _compute_reward(self, decision_event, snapshot_list):\n",
    "        start_tick = decision_event.tick + 1\n",
    "        end_tick = decision_event.tick + self._time_window\n",
    "        ticks = list(range(start_tick, end_tick))\n",
    "\n",
    "        # Calculate truncate reward.\n",
    "        future_fulfillment = snapshot_list[\"ports\"][ticks::\"fulfillment\"]\n",
    "        future_shortage = snapshot_list[\"ports\"][ticks::\"shortage\"]\n",
    "        decay_list = [self._time_decay_factor ** i for i in range(end_tick - start_tick)\n",
    "                      for _ in range(future_fulfillment.shape[0]//(end_tick-start_tick))]\n",
    "\n",
    "        tot_fulfillment = np.dot(future_fulfillment, decay_list)\n",
    "        tot_shortage = np.dot(future_shortage, decay_list)\n",
    "\n",
    "        return np.float(self._fulfillment_factor * tot_fulfillment - self._shortage_factor * tot_shortage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Agent](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#agent)\n",
    "\n",
    "For this scenario, the agent is the abstraction of a port. We choose DQN as our underlying learning algorithm with a TD-error-based sampling mechanism.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.rl import AbsAgent, ColumnBasedStore\n",
    "\n",
    "\n",
    "class CIMAgent(AbsAgent):\n",
    "    def __init__(self, name, algorithm, experience_pool: ColumnBasedStore, min_experiences_to_train,\n",
    "                 num_batches, batch_size):\n",
    "        super().__init__(name, algorithm, experience_pool)\n",
    "        self._min_experiences_to_train = min_experiences_to_train\n",
    "        self._num_batches = num_batches\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "    def train(self):\n",
    "        if len(self._experience_pool) < self._min_experiences_to_train:\n",
    "            return\n",
    "\n",
    "        for _ in range(self._num_batches):\n",
    "            indexes, sample = self._experience_pool.sample_by_key(\"loss\", self._batch_size)\n",
    "            state = np.asarray(sample[\"state\"])\n",
    "            action = np.asarray(sample[\"action\"])\n",
    "            reward = np.asarray(sample[\"reward\"])\n",
    "            next_state = np.asarray(sample[\"next_state\"])\n",
    "            loss = self._algorithm.train(state, action, reward, next_state)\n",
    "            self._experience_pool.update(indexes, {\"loss\": loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Agent Manager](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#agent-manager)\n",
    "\n",
    "The agent manager inherits from MARO's `AbsAgentManager` which is an agent assembler and isolates the complexities of the environment and algorithm. It will load the DQN algorithm and an experience pool for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import yaml\n",
    "\n",
    "import torch.nn as\n",
    "from torch.nn.functional import smooth_l1_loss\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "from maro.rl import SimpleAgentManager, LearningModel, FullyConnectedNet, DQN, DQNHyperParams, ColumnBasedStore\n",
    "\n",
    "\n",
    "input_dim = 171\n",
    "num_actions = 21\n",
    "\n",
    "\n",
    "\n",
    "def create_dqn_agents(agent_id_list):\n",
    "    agent_dict = {}\n",
    "    for agent_id in agent_id_list:\n",
    "        eval_model = LearningModel(\n",
    "            decision_layers=FullyConnectedNet(\n",
    "                name=f'{agent_id}.policy',\n",
    "                input_dim=input_dim,\n",
    "                output_dim=num_actions,\n",
    "                activation=nn.LeakyReLU, \n",
    "                hidden_dims=[256, 128, 64],\n",
    "                softmax_enabled=False,\n",
    "                batch_norm_enabled=True,\n",
    "                dropout_p=.0\n",
    "            )\n",
    "        )\n",
    "\n",
    "        algorithm = DQN(\n",
    "            eval_model=eval_model,\n",
    "            optimizer_cls=RMSprop,\n",
    "            optimizer_params={\"lr\": 0.05},\n",
    "            loss_func=nn.functional.smooth_l1_loss,\n",
    "            hyper_params=DQNHyperParams(\n",
    "                num_actions=num_actions,\n",
    "                reward_decay=.0,\n",
    "                target_replacement_frequency=5,\n",
    "                tau=0.1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        experience_pool = ColumnBasedStore()\n",
    "        agent_dict[agent_id] = CIMAgent(\n",
    "            name=agent_id,\n",
    "            algorithm=algorithm,\n",
    "            experience_pool=experience_pool,\n",
    "            min_experiences_to_train=1024,\n",
    "            num_batches=10,\n",
    "            batch_size=128\n",
    "        )\n",
    "\n",
    "    return agent_dict\n",
    "\n",
    "\n",
    "class DQNAgentManager(SimpleAgentManager):\n",
    "    def train(self, experiences_by_agent, performance=None):\n",
    "        self._assert_train_mode()\n",
    "\n",
    "        # store experiences for each agent\n",
    "        for agent_id, exp in experiences_by_agent.items():\n",
    "            exp.update({\"loss\": [1e8] * len(list(exp.values())[0])})\n",
    "            self.agent_dict[agent_id].store_experiences(exp)\n",
    "\n",
    "        for agent in self.agent_dict.values():\n",
    "            agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop with [Actor and Learner](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#learner-and-actor)\n",
    "\n",
    "This code cell demonstrates the typical workflow of a learning policy's interaction with a MARO environment. \n",
    "\n",
    "- Initialize an environment with specific scenario and topology parameters. \n",
    "\n",
    "- Define scenario-specific components, e.g. shapers. \n",
    "\n",
    "- Create an agent manager, which assembles underlying agents. \n",
    "\n",
    "- Create an actor and a learner to start the training process in which the agent manager interacts with the environment for collecting experiences and updating policies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f571875d11b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m agent_manager = DQNAgentManager(name=\"cim_learner\",\n\u001b[1;32m     24\u001b[0m                                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAgentManagerMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN_INFERENCE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                 \u001b[0magent_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_dqn_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_id_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                                 \u001b[0mstate_shaper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_shaper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                 \u001b[0maction_shaper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_shaper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-06b292b9e726>\u001b[0m in \u001b[0;36mcreate_dqn_agents\u001b[0;34m(agent_id_list)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mhidden_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0msoftmax_enabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "from maro.simulator import Env\n",
    "from maro.rl import SimpleLearner, SimpleActor, AgentManagerMode, TwoPhaseLinearExplorer\n",
    "from maro.utils import Logger, LogFormat\n",
    "\n",
    "# Step 1: initialize a CIM environment for using a toy dataset. \n",
    "env = Env(\"cim\", \"toy.4p_ssdd_l0.0\", durations=1120)\n",
    "agent_id_list = [str(agent_id) for agent_id in env.agent_idx_list]\n",
    "\n",
    "# Step 2: create state, action and experience shapers. We also need to create an explorer here due to the \n",
    "# greedy nature of the DQN algorithm.  \n",
    "state_shaper = CIMStateShaper(look_back=7, max_ports_downstream=2, \n",
    "                              port_attributes=[\"empty\", \"full\", \"on_shipper\", \"on_consignee\", \n",
    "                                               \"booking\", \"shortage\", \"fulfillment\"],\n",
    "                              vessel_attributes=[\"empty\", \"full\", \"remaining_space\"]\n",
    "                             )\n",
    "\n",
    "action_shaper = CIMActionShaper(action_space=list(np.linspace(-1.0, 1.0, num_actions)))\n",
    "\n",
    "experience_shaper = TruncatedExperienceShaper(time_window=100, fulfillment_factor=1.0, shortage_factor=1.0,\n",
    "                                              time_decay_factor=0.97)\n",
    "\n",
    "# Step 3: create an agent manager.\n",
    "agent_manager = DQNAgentManager(name=\"cim_learner\",\n",
    "                                mode=AgentManagerMode.TRAIN_INFERENCE,\n",
    "                                agent_dict=create_dqn_agents(agent_id_list),\n",
    "                                state_shaper=state_shaper,\n",
    "                                action_shaper=action_shaper,\n",
    "                                experience_shaper=experience_shaper)\n",
    "\n",
    "# Step 4: Create an actor and a learner to start the training process. \n",
    "actor = SimpleActor(env, agent_manager)\n",
    "learner = SimpleLearner(trainable_agents=agent_manager, actor=actor, \n",
    "                        explorer=TwoPhaseLinearExplorer(eps_split=0.32, progress_split=0.5, max_eps=0.4, min_eps=.0),\n",
    "                        logger=Logger(\"single_host_cim_learner\", format_=LogFormat.simple, auto_timestamp=False))\n",
    "\n",
    "learner.train(max_episode=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
